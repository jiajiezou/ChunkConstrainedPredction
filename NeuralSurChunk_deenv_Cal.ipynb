{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "706aeb97",
      "metadata": {},
      "source": [
        "# Calculating Neural-Surprisal Correlation and Boundary Effects in the ECoG Dataset\n",
        "\n",
        "This script computes the neural-surprisal correlation using the [open ECoG dataset](https://doi.org/10.1101/2025.02.14.638352).\n",
        "\n",
        "The analysis involves two main steps:\n",
        "\n",
        "- **Regressing out baseline auditory responses** using a temporal response function.\n",
        "- **Calculating neural-surprisal correlation** for sentence-initial and non-initial words using Spearman correlation (with balanced sample sizes)\n",
        "\n",
        "The computed results are stored in the `Calculated/` folder, and visualizations are provided in the `NeuralSurChunk_deenv_Vis.ipynb` notebook.  \n",
        "Note: Due to randomization in the processing steps, results may vary slightly between runs. However, the main findings (e.g., chunking constrains prediction) remain consistent.\n",
        "\n",
        "The preprocessing code is adapted from the open-source [podcast-ecog-tutorials](https://github.com/hassonlab/podcast-ecog-tutorials).  \n",
        "We thank Zada et al. for publicly releasing the ECoG dataset and the tutorials that made this analysis possible.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aea07e26",
      "metadata": {},
      "source": [
        "## package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2a4659b",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\InstalledSoftware\\Anaconda\\envs\\py39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Device set to use cpu\n",
            "d:\\InstalledSoftware\\Anaconda\\envs\\py39\\lib\\site-packages\\transformers\\pipelines\\token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"none\"` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import mne\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from nilearn.plotting import plot_markers\n",
        "from mne_bids import BIDSPath\n",
        "\n",
        "from himalaya.backend import set_backend, get_backend\n",
        "\n",
        "import pickle\n",
        "\n",
        "from scipy import stats \n",
        "from scipy import signal\n",
        "from scipy.signal import find_peaks\n",
        "from scipy.io import wavfile\n",
        "np.random.seed(25)\n",
        "\n",
        "import warnings\n",
        "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
        "warnings.simplefilter('ignore', ConvergenceWarning)\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from himalaya.scoring import correlation_score\n",
        "from himalaya.ridge import RidgeCV\n",
        "# from voxelwise_tutorials.delayer import Delayer\n",
        "\n",
        "# The Windows platform is not supported by the original voxelwise_tutorials package.\n",
        "# Manually define the Delayer from the voxelwise_tutorials toolbox.\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.utils.validation import check_is_fitted, check_array\n",
        "class Delayer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Scikit-learn Transformer to add delays to features.\n",
        "\n",
        "    This assumes that the samples are ordered in time.\n",
        "    Adding a delay of 0 corresponds to leaving the features unchanged.\n",
        "    Adding a delay of 1 corresponds to using features from the previous sample.\n",
        "\n",
        "    Adding multiple delays can be used to take into account the slow\n",
        "    hemodynamic response, with for example `delays=[1, 2, 3, 4]`.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    delays : array-like or None\n",
        "        Indices of the delays applied to each feature. If multiple values are\n",
        "        given, each feature is duplicated for each delay.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    n_features_in_ : int\n",
        "        Number of features seen during the fit.\n",
        "\n",
        "    Example\n",
        "    -------\n",
        "    >>> from sklearn.pipeline import make_pipeline\n",
        "    >>> from voxelwise_tutorials.delayer import Delayer\n",
        "    >>> from himalaya.kernel_ridge import KernelRidgeCV\n",
        "    >>> pipeline = make_pipeline(Delayer(delays=[1, 2, 3, 4]), KernelRidgeCV())\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, delays=None):\n",
        "        self.delays = delays\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Fit the delayer.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array of shape (n_samples, n_features)\n",
        "            Training data.\n",
        "\n",
        "        y : array of shape (n_samples,) or (n_samples, n_targets)\n",
        "            Target values. Ignored.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        self : returns an instance of self.\n",
        "        \"\"\"\n",
        "        X = self._validate_data(X, dtype='numeric')\n",
        "        self.n_features_in_ = X.shape[1]\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Transform the input data X, copying features with different delays.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array of shape (n_samples, n_features)\n",
        "            Input data.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Xt : array of shape (n_samples, n_features * n_delays)\n",
        "            Transformed data.\n",
        "        \"\"\"\n",
        "        check_is_fitted(self)\n",
        "        X = check_array(X, copy=True)\n",
        "\n",
        "        n_samples, n_features = X.shape\n",
        "        if n_features != self.n_features_in_:\n",
        "            raise ValueError(\n",
        "                'Different number of features in X than during fit.')\n",
        "\n",
        "        if self.delays is None:\n",
        "            return X\n",
        "\n",
        "        X_delayed = np.zeros((n_samples, n_features * len(self.delays)),\n",
        "                             dtype=X.dtype)\n",
        "        for idx, delay in enumerate(self.delays):\n",
        "            beg, end = idx * n_features, (idx + 1) * n_features\n",
        "            if delay == 0:\n",
        "                X_delayed[:, beg:end] = X\n",
        "            elif delay > 0:\n",
        "                X_delayed[delay:, beg:end] = X[:-delay]\n",
        "            elif delay < 0:\n",
        "                X_delayed[:-abs(delay), beg:end] = X[abs(delay):]\n",
        "\n",
        "        return X_delayed\n",
        "\n",
        "    def reshape_by_delays(self, Xt, axis=1):\n",
        "        \"\"\"Reshape an array, splitting and stacking across delays.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        Xt : array of shape (n_samples, n_features * n_delays)\n",
        "            Transformed array.\n",
        "        axis : int, default=1\n",
        "            Axis to split.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Xt_split :array of shape (n_delays, n_samples, n_features)\n",
        "            Reshaped array, splitting across delays.\n",
        "        \"\"\"\n",
        "        delays = self.delays or [0]  # deals with None\n",
        "        return np.stack(np.split(Xt, len(delays), axis=axis))\n",
        "\n",
        "# Model to restore punctuation in the text\n",
        "from deepmultilingualpunctuation import PunctuationModel\n",
        "punc_model = PunctuationModel('/data/model/oliverguhr/fullstop-punctuation-multilang-large')\n",
        "# punc_model = PunctuationModel('F:/Model/oliverguhr/fullstop-punctuation-multilang-large')\n",
        "\n",
        "backend = set_backend(\"numpy\")\n",
        "def FontSetting():\n",
        "    plt.rcParams.update(\n",
        "        {'font.size': 10,\n",
        "          'font.family': ['Arial'],\n",
        "          'font.weight': 'normal',\n",
        "          # 'legend.fontsize': 'x-small',\n",
        "          'legend.fontsize': 'medium',\n",
        "          'axes.labelsize': 'Large',\n",
        "          'axes.titlesize': 'Large',\n",
        "          'axes.titleweight': 'normal',\n",
        "          })\n",
        "    plt.rcParams['svg.fonttype'] = 'none'\n",
        "FontSetting()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72bd63bd",
      "metadata": {},
      "source": [
        "## load features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d68b0cb",
      "metadata": {},
      "source": [
        "### load surprisal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fedbba7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model accuracy: 30.942%\n"
          ]
        },
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "word_idx",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "word",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "Surp",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "entropy",
                  "rawType": "float64",
                  "type": "float"
                }
              ],
              "conversionMethod": "pd.DataFrame",
              "ref": "7de9e9ef-6fd2-4b2d-aa70-2b32efb2f962",
              "rows": [
                [
                  "0",
                  "Act",
                  "4.918057224093667",
                  "2.402717"
                ],
                [
                  "1",
                  "one,",
                  "4.6910661299529055",
                  "3.732053"
                ],
                [
                  "2",
                  "monkey",
                  "4.7434132835507405",
                  "6.6212687"
                ],
                [
                  "3",
                  "in",
                  "2.3729393424945395",
                  "4.4448376"
                ],
                [
                  "4",
                  "the",
                  "0.4064864743173802",
                  "3.7816224"
                ]
              ],
              "shape": {
                "columns": 3,
                "rows": 5
              }
            },
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>Surp</th>\n",
              "      <th>entropy</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>word_idx</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Act</td>\n",
              "      <td>4.918057</td>\n",
              "      <td>2.402717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>one,</td>\n",
              "      <td>4.691066</td>\n",
              "      <td>3.732053</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>monkey</td>\n",
              "      <td>4.743413</td>\n",
              "      <td>6.621269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>in</td>\n",
              "      <td>2.372939</td>\n",
              "      <td>4.444838</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>the</td>\n",
              "      <td>0.406486</td>\n",
              "      <td>3.781622</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            word      Surp   entropy\n",
              "word_idx                            \n",
              "0            Act  4.918057  2.402717\n",
              "1           one,  4.691066  3.732053\n",
              "2         monkey  4.743413  6.621269\n",
              "3             in  2.372939  4.444838\n",
              "4            the  0.406486  3.781622"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bids_root = \"YourPathToDataset\" \n",
        "# ---------- contextual measures\n",
        "# Load transcript and surprisal\n",
        "transcript_path = f\"{bids_root}stimuli/gpt2-xl/transcript.tsv\"\n",
        "\n",
        "df_contextual = pd.read_csv(transcript_path, sep=\"\\t\", index_col=0)\n",
        "df_contextual['Surp'] = -np.log10(np.array(df_contextual['true_prob']) + 1e-9)\n",
        "if \"rank\" in df_contextual.columns:\n",
        "    model_acc = (df_contextual[\"rank\"] == 0).mean()\n",
        "    print(f\"Model accuracy: {model_acc*100:.3f}%\")\n",
        "\n",
        "\n",
        "# words and correspd surprisal\n",
        "df_word = df_contextual.groupby(\"word_idx\").agg(dict(word=\"first\", start=\"first\", end=\"last\"))\n",
        "\n",
        "df_word_sur = df_contextual.groupby(\"word_idx\").agg(dict(word=\"first\", Surp=\"sum\", entropy=\"first\"))\n",
        "df_word_sur.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "284f94a5",
      "metadata": {},
      "source": [
        "## Reading MEG and modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60b4481f",
      "metadata": {},
      "source": [
        "### functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bcd7f17",
      "metadata": {},
      "outputs": [],
      "source": [
        "def Epoching(raw, events):\n",
        "    # epoch\n",
        "    epochs = mne.Epochs(\n",
        "        raw,\n",
        "        events,\n",
        "        tmin=-2,\n",
        "        tmax=2,\n",
        "        baseline=None,\n",
        "        proj=False,\n",
        "        event_id=None,\n",
        "        preload=True,\n",
        "        event_repeated=\"merge\",\n",
        "    )\n",
        "    \n",
        "    print(f\"Epochs object has a shape of: {epochs._data.shape}\")\n",
        "    return epochs\n",
        "\n",
        "def DataReading(subj, bids_root, df_word, fs_re):\n",
        "    file_path = BIDSPath(root=f\"{bids_root}derivatives/ecogprep\",\n",
        "                        subject=subj, \n",
        "                        task=\"podcast\", datatype=\"ieeg\", description=\"highgamma\",\n",
        "                        suffix=\"ieeg\", extension=\".fif\")\n",
        "    print(f\"File path within the dataset: {file_path}\")\n",
        "\n",
        "    # You only need to run this if using Colab (i.e. if you did not set bids_root to a local directory)\n",
        "    if not len(bids_root):\n",
        "        !wget -nc https://s3.amazonaws.com/openneuro.org/ds005574/$file_path\n",
        "        file_path = file_path.basename\n",
        "\n",
        "    # data loading\n",
        "    raw = mne.io.read_raw_fif(file_path, verbose=False)\n",
        "    print(len(raw.ch_names))\n",
        "\n",
        "    # get event\n",
        "    events = np.zeros((len(df_word), 3), dtype=int)\n",
        "    events[:, 0] = (df_word.start * raw.info['sfreq']).astype(int)\n",
        "    print(events.shape)\n",
        "\n",
        "    # resample\n",
        "    raw, events = raw.resample(fs_re, events=events)\n",
        "\n",
        "    # epoch\n",
        "    epochs = Epoching(raw, events)\n",
        "    \n",
        "    return raw, epochs, events\n",
        "    \n",
        "def Boundaries(punc_model, epochs, df_word_sur):\n",
        "    # recover punc and label boundaries\n",
        "    pass_str = ' '.join(df_word_sur.iloc[epochs.selection].word)\n",
        "    result = punc_model.restore_punctuation(pass_str)\n",
        "    word_punc = result.split(' ')\n",
        "    ini_idx = np.array([tmp_i+1 for tmp_i, tmp in enumerate(word_punc) \n",
        "                        if tmp.endswith((',', '!', '.', '?', ':'))][:-1])\n",
        "    non_ini_idx = np.array([tmp_i+1 for tmp_i, tmp in enumerate(word_punc) \n",
        "                            if not tmp.endswith((',', '!', '.', '?', ':'))][:-1])\n",
        "\n",
        "    return ini_idx, non_ini_idx\n",
        "\n",
        "def ModelInit(fs_):\n",
        "    lambdas = [1e-20] \n",
        "    # In this analysis, regularization does not improve modeling performance.\n",
        "    # Instead, it tends to shrink the predicted signal, making the acoustic responses\n",
        "    # remain in the residuals. Therefore, we set the regularization parameter \n",
        "    # to a very small value. \n",
        "    inner_cv = KFold(n_splits=5, shuffle=False)\n",
        "    # ridge regression\n",
        "    scaler = StandardScaler(with_mean=False, with_std=True)\n",
        "    delayer = Delayer(delays=np.arange(-int(fs_*1.5), int(fs_*1.5)+1))\n",
        "    solver_params = dict(n_alphas_batch=10)\n",
        "\n",
        "    ridge = RidgeCV(\n",
        "        alphas=lambdas, cv=inner_cv, \n",
        "        solver_params=solver_params, fit_intercept=True)\n",
        "    pipeline = Pipeline([\n",
        "        ('scaler', scaler),\n",
        "        ('delayer', delayer),\n",
        "        ('ridgecv', ridge)\n",
        "        ])\n",
        "    return pipeline\n",
        "\n",
        "def TRFRegre(X, Y, fs_):\n",
        "    # X: time x feas\n",
        "    # Y: time x outputs\n",
        "    outer_cv = KFold(n_splits=2)\n",
        "    h_hat_s = []\n",
        "    corrs = []\n",
        "    for train, test in outer_cv.split(Y):\n",
        "        pipeline = ModelInit(fs_)\n",
        "        pipeline.fit(X[train], Y[train])\n",
        "        Y_hat = pipeline.predict(X[test])\n",
        "        # cc\n",
        "        corr_tmp = correlation_score(Y[test], Y_hat)\n",
        "        corrs.append(corr_tmp)\n",
        "\n",
        "        # waveform\n",
        "        ridge_fitted = pipeline['ridgecv']\n",
        "\n",
        "        h_hat = np.transpose(\n",
        "            np.reshape(ridge_fitted.coef_, \n",
        "            (X.shape[-1], -1, Y.shape[-1]), order='F'),\n",
        "            [1, 0, 2])\n",
        "    \n",
        "        h_hat_s.append(h_hat)\n",
        "    cc = np.stack(corrs).mean(0)\n",
        "    return cc, h_hat_s, pipeline\n",
        "\n",
        "\n",
        "def discrete_envelope(envelope):\n",
        "    peaks, _ = find_peaks(envelope)\n",
        "    gradient = np.gradient(envelope, 1/100)\n",
        "    gradient[gradient<0]=0\n",
        "    peaks_grad,_ = find_peaks(gradient)\n",
        "\n",
        "    vec_peaks = np.zeros(len(gradient))\n",
        "    vec_peaks[peaks] = envelope[peaks]\n",
        "\n",
        "    vec_grad = np.zeros(len(gradient))\n",
        "    vec_grad[peaks_grad] = gradient[peaks_grad]\n",
        "    final_peaks = vec_peaks\n",
        "    final_gradient = vec_grad\n",
        "    return final_peaks, final_gradient\n",
        "\n",
        "def get_env(fs_re=32):\n",
        "    audio_path = f\"{bids_root}stimuli/podcast.wav\"\n",
        "    highfs, highqa = wavfile.read(audio_path)\n",
        "    if highqa.ndim > 1:\n",
        "        highqa = highqa[:, 0]  # take first channel\n",
        "    env = signal.resample(abs(highqa), num=round(highqa.size / highfs * fs_re))\n",
        "\n",
        "    env_peaks, env_gradient = discrete_envelope(env)\n",
        "\n",
        "    return env, env_peaks, env_gradient\n",
        "\n",
        "def TRFControlAcoustic(raw, epochs, events, fs_re, subj, non_ini_idx, ini_idx):\n",
        "    # ===regress out envelope and word boundaries\n",
        "    f_tmp1, axes_tmp1 = plt.subplots(2, 3, layout=\"constrained\", figsize=(8, 6))\n",
        "\n",
        "    # ERP: before regression\n",
        "    epochs[non_ini_idx].average().apply_baseline(baseline=(-2, -1)).plot(\n",
        "        show=False, axes=axes_tmp1[0, 0])\n",
        "    epochs[ini_idx].average().apply_baseline(baseline=(-2, -1)).plot(\n",
        "        show=False, axes=axes_tmp1[1, 0])\n",
        "    \n",
        "    # signal\n",
        "    cont_data = raw.get_data()\n",
        "    # features\n",
        "    env, env_peaks, env_gradient = get_env(fs_re)\n",
        "    w_onset = np.zeros(env.shape)\n",
        "    s_onset = np.zeros(env.shape)\n",
        "\n",
        "    w_onset_idx = events[epochs.selection]\n",
        "    w_onset[w_onset_idx[non_ini_idx]] = 1\n",
        "\n",
        "    s_onset_idx = w_onset_idx[ini_idx]\n",
        "    s_onset[s_onset_idx] = 1\n",
        "\n",
        "    contr_feas = np.array([env_peaks, env_gradient, w_onset, s_onset])\n",
        "\n",
        "    # modeling\n",
        "    print('===================== Modeling ==================')\n",
        "    cc, h_hat_s, pipe_trf_simu = TRFRegre(\n",
        "        contr_feas.T, cont_data.T, \n",
        "        fs_re)\n",
        "    cont_data_hat_T = pipe_trf_simu.predict(contr_feas.T).T\n",
        "\n",
        "    # predicted signal\n",
        "    print('===================== Visualization ==================')\n",
        "    raw1 = mne.io.RawArray(cont_data_hat_T, raw.info)\n",
        "    epochs_pred = Epoching(raw1, events)\n",
        "    epochs_pred[non_ini_idx].average().apply_baseline(baseline=(-2, -1)).plot(\n",
        "        show=False, axes=axes_tmp1[0, 1])\n",
        "    epochs_pred[ini_idx].average().apply_baseline(baseline=(-2, -1)).plot(\n",
        "        show=False, axes=axes_tmp1[1, 1])\n",
        "\n",
        "    # residual\n",
        "    resi_data = cont_data - cont_data_hat_T\n",
        "\n",
        "    raw2 = mne.io.RawArray(resi_data, raw.info)\n",
        "    epochs_resi = Epoching(raw2, events)\n",
        "    epochs_resi[non_ini_idx].average().apply_baseline(baseline=(-2, -1)).plot(\n",
        "        show=False, axes=axes_tmp1[0, 2])\n",
        "    epochs_resi[ini_idx].average().apply_baseline(baseline=(-2, -1)).plot(\n",
        "        show=False, axes=axes_tmp1[1, 2])\n",
        "\n",
        "    # TRF\n",
        "    h_hat_s = backend.to_numpy(h_hat_s)\n",
        "    h_hat_s_mean = np.stack(h_hat_s).mean(axis=0)\n",
        "    f_tmp2, axs_tmp2 = plt.subplots(1, h_hat_s_mean.shape[1], figsize=(10, 3))\n",
        "    for fea_i in range(h_hat_s_mean.shape[1]):\n",
        "        evoked_trf = mne.EvokedArray(h_hat_s_mean[:,fea_i,:].T, raw.info, tmin=-1.5)\n",
        "        evoked_trf.plot(show=False, axes=axs_tmp2[fea_i])\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    f_tmp1.savefig(f'./intermediate_plot/TRF_signal_{subj}.png')\n",
        "    f_tmp2.savefig(f'./intermediate_plot/TRF_{subj}.png')\n",
        "\n",
        "    raw._data = resi_data\n",
        "    epochs = Epoching(raw, events)\n",
        "\n",
        "    return raw, epochs\n",
        "\n",
        "def SignalSurp(epochs, df_word_sur):\n",
        "    epochs_data = epochs.get_data() # electrodes * number of lags\n",
        "    epochs_data = epochs_data.reshape((len(epochs), -1), order='F')\n",
        "    Y = epochs_data\n",
        "\n",
        "    if \"torch\" in get_backend().__name__:\n",
        "        Y = Y.astype(np.float32)\n",
        "\n",
        "    # surprisal\n",
        "    X_s = df_word_sur.iloc[epochs.selection].loc[:, ['Surp']].values\n",
        "    return Y, X_s\n",
        "\n",
        "def closest_elements(A, B):\n",
        "    diff_tmp = abs(A[:, np.newaxis] - B[np.newaxis, :])\n",
        "    best_idx = []\n",
        "    for base_i in range(len(B)):\n",
        "        min_r_tmp = np.nanargmin(diff_tmp[:, base_i])\n",
        "        diff_tmp[min_r_tmp, :] = np.nan\n",
        "        best_idx.append(min_r_tmp)\n",
        "    best_idx = np.array(best_idx)\n",
        "\n",
        "    return best_idx\n",
        "\n",
        "def CorrelationAna(X_s, Y, non_ini_idx, ini_idx):\n",
        "    se_i = 0\n",
        "    # correlation\n",
        "    cc_surp = [stats.spearmanr(X_s[:, se_i], tmp)[0] for tmp in Y.T]\n",
        "\n",
        "    # For noninitial words: randomly select a subset to balance the sample size.\n",
        "    non_ini_idx_bala = np.random.choice(non_ini_idx, (101, ini_idx.shape[0]))\n",
        "    cc_non_surp = [\n",
        "        [stats.spearmanr(X_s[tmp_idx, se_i], tmp[tmp_idx])[0] \n",
        "         for tmp in Y.T]\n",
        "         for tmp_idx in non_ini_idx_bala\n",
        "         ]\n",
        "    # We also sampled non-initial words to balance the surprisal distribution \n",
        "    # between initial and non-initial words. The boundary effect remains similar \n",
        "    # when the surprisal distribution is balanced.\n",
        "    distr_idx = closest_elements(X_s[non_ini_idx, se_i], X_s[ini_idx, se_i])\n",
        "    cc_non_surp_distr = [\n",
        "        stats.spearmanr(X_s[non_ini_idx, se_i][distr_idx], tmp[non_ini_idx][distr_idx])[0] \n",
        "        for tmp in Y.T]\n",
        "    \n",
        "    # for initial words\n",
        "    cc_ini_surp = [stats.spearmanr(X_s[ini_idx, se_i], tmp[ini_idx])[0] for tmp in Y.T]\n",
        "    cc_surp = np.array([cc_surp]+cc_non_surp+[cc_non_surp_distr]+[cc_ini_surp]).T\n",
        "\n",
        "    return cc_surp"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65a006ec",
      "metadata": {},
      "source": [
        "### Main script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8744c7d",
      "metadata": {},
      "outputs": [],
      "source": [
        "subjs = [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\"]\n",
        "fs_re = 32\n",
        "f_subjs, axes = plt.subplots(2, len(subjs), layout=\"constrained\", figsize=(20, 5))\n",
        "for subj_i, subj in enumerate(subjs):\n",
        "    print(f'********** subj: {subj}, Dataloading **************')\n",
        "    raw, epochs, events = DataReading(\n",
        "        subj, bids_root, df_word, fs_re)\n",
        "\n",
        "    print(f'********** subj: {subj}, Regress out envelope **************')\n",
        "    ini_idx, non_ini_idx = Boundaries(punc_model, epochs, df_word_sur)\n",
        "\n",
        "    raw, epochs = TRFControlAcoustic(\n",
        "        raw, epochs, events, fs_re, subj, non_ini_idx, ini_idx)\n",
        "    \n",
        "    print(f'********** subj: {subj}, Preparing features for modeling **************')\n",
        "    Y, X_s = SignalSurp(epochs, df_word_sur)\n",
        "\n",
        "    print(f'********** subj: {subj}, Neural-Surprisal correlation **************')\n",
        "    cc_surp = CorrelationAna(X_s, Y, non_ini_idx, ini_idx)\n",
        "\n",
        "    # saving....\n",
        "    ch_name = [ch['ch_name'] for ch in raw.info['chs']]\n",
        "    ch_name = np.array(ch_name)\n",
        "\n",
        "    coords = np.vstack([ch['loc'][:3] for ch in raw.info['chs']])\n",
        "    coords *= 1000  # nilearn likes to plot in meters, not mm\n",
        "    coords = np.array(coords)\n",
        "\n",
        "    data_saving = {'cc_surp':cc_surp, 'ch_name':ch_name, 'coords':coords}\n",
        "    with open(f\"./Calculated/NeuralSurp_{subj}.pickle\", \"wb\") as output_file:\n",
        "        pickle.dump(data_saving, output_file)\n",
        "plt.show()\n",
        "f_subjs.savefig('./intermediate_plot/ERP.png', dpi=300)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "jmRhBf5dJLIZ",
        "ad734fb9-6bf7-435c-8aa4-860098707880",
        "h3gmhh7nLRWS"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py39",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
